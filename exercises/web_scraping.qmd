---
title: "Scraping Star Wars movies and an E-Commerce website"
format: 
  live-html:
    toc: true
engine: knitr
webr:
  packages: 
    - tidyverse
    - rvest
    - polite
  cell-options:
    autorun: false
  resources:
    - https://rvest.tidyverse.org/articles/starwars.html
    - https://raw.githubusercontent.com/data-science-with-r/data-tidying-import/main/exercises/data/items-80.csv
---

<!-- begin: webr fodder -->

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}

```{webr}
#| edit: false
#| echo: false
#| output: false
options("readr.edition" = 1)
```

<!-- end: webr fodder -->

```{r}
#| include: false
library(tidyverse)
library(rvest)
library(polite)
ecomm <- read_csv("data/items-80.csv")
starwars <- read_html("https://rvest.tidyverse.org/articles/starwars.html")
```

We'll use the following packages for this programming exercise:

```{webr}
#| message: false
#| warning: false
library(tidyverse)
library(rvest)
library(polite)
```

## Star Wars

Scraping data is defined as extracting data from one source to another.
We are going to use the following [website](https://rvest.tidyverse.org/articles/starwars.html) to practice scraping data; taking data on the website and transporting it into a more workable format with R code.

### Scraping titles

Please visit the [website](https://rvest.tidyverse.org/articles/starwars.html) and identify the movie titles.
What are they?

::: {.callout-tip collapse="true"}
## Solution

There are seven movie titles on this page.
They include:

-- The Phantom Menace

-- Attack of the Clones

-- Revenge of the Sith

-- A New Hope

-- The Empire Strikes Back

-- Return of the Jedi

-- The Force Awakens
:::

The next step is to identify the html element that is associated with each title.
To help us, we can use [Selector Gadget](https://selectorgadget.com/).
Selector Gadget is an open source Chrome Extension that helps users find elements of an html page easier.
This demonstration will use Selector Gadget.
To install Selector Gadget, please go to the following [page](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en&pli=1).
When the Selector Gadget is installed, you should see ![](images/gadget.png) in your task bar.

#### Start by reading a HTML page with read_html()

First, we need to reference the website.
We can do this by using `read_html()` from the rvest package.
Run the following code below to do so.

```{webr}
starwars <- read_html("https://rvest.tidyverse.org/articles/starwars.html")
```

#### Find the element

Next, we need to find the element that is associated with the titles of each Star Wars movie.
Click the ![](images/gadget.png) icon and click on one of the titles. Click on another one.

![](images/element1.png)

Notice that the next you click on (that you wish to scrape) is highlighted in green. Text in yellow show what else has the same element tag as the text you are clicking on.

We can see that the text in green/yellow include the seven titles, and the "ON THIS PAGE" text. If we don't want to scrape certain text (e.g., the menu title), we can click it, changing the yellow box to red.

![](images/element2.png)

Now, we have our seven titles in green/yellow that we want to scrape. In the bottom right corner, we can see the element associated with just these seven titles. We will use this element information to scrape the titles below.

```{webr}
starwars |>
  html_elements("#main h2") |>
  html_text2()
```

Change the above code from `html_text2()` to `html_text()` to best understand the difference between these two functions. 

::: {.callout-tip collapse="true"}
## Solution

See the help file for each function [here](https://rvest.tidyverse.org/reference/html_text.html).

html_text() returns raw underlying text, including white pace and other symbols. html_text2() tries to simulate how the text is displayed.
:::

### Scraping release dates

Suppose now we want to scrape release dates instead. We can follow the same process as above. 

Click on a released date that you hope to scrape. Click on any yellow text you wish to exclude that has the same element tag. Click on a second released date. Again, click on any yellow text you wish to exclude.

Once you completed this process, you should see an element tag for just the released dates of *section > p:nth-child(2)*. Use this information to scrape the seven released dates below.

![](images/element-released.png)

```{webr}
#insert code here
```


::: {.callout-tip collapse="true"}
## Solution
```{r}
starwars |>
  html_elements("section > p:nth-child(2)") |>
  html_text2()
```
:::

## Working with scraped eCommerce pages

```{webr}
ecomm <- read_csv("items-80.csv")
```

In the last code along activity, you wrote a function to scrape eCommerce data from multiple pages.
Here, we are going to practice working with numerical and text data.
The data set name is called `ecomm`.

Take a glimpse at the `ecomm` data set below to familiarize yourself with the variables.
What are they?

```{webr}
# insert code here
```

::: {.callout-tip collapse="true"}
## Solution

```{r}
glimpse(ecomm)
```

There is a `title`, `url` and `price` variable.
:::

Now, let's start by making a histogram of price using tools from the `ggplot` package in the `tidyverse`.
Hint: Use the functions [ggplot()](https://ggplot2.tidyverse.org/reference/ggplot.html) and [geom_histogram()](https://ggplot2.tidyverse.org/reference/geom_histogram.html) to make your visualization below.
Note: Play around with the binwidth, and add appropriate labels if able!

```{webr}
# insert code here
```

::: {.callout-tip collapse="true"}
## Solution

```{r}
ggplot(ecomm, aes(x = price)) +
  geom_histogram(binwidth = 5) + 
  labs(
    title = "Price of eCommerse scraped data",
    x = "Price"
  )
```
:::

Now, let's work with some of the text data to discover more patterns.
Suppose you wonder if an eCommerce business that sells shorts is cheaper than others.
First, we need to find how many listings contain the word "short".
We can do this using [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) from the `stringR` package.
Run the following code.
What is it doing?

```{webr}
ecomm <- ecomm |>
  mutate(dummy = ifelse(str_detect(title, "Short"), "short", "else"))

ecomm
```

::: {.callout-tip collapse="true"}
## Solution

This code is using `mutate()` to create a new dummy variable called `dummy`.
It is using a combination of `ifelse()` and `str_detect()` to look into the title column, and see if the string "Short" was detected or not.
If yes, the new variable `dummy` will contain "short" for that row.
If no, the new variable will contain "else".
:::

We just made a dummy (or indicator) variable!
This is going to help us see if eCommerce businesses that sell shorts are, on average, more or less expensive than others.
Use this variable, along with the `dplyr` functions of [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) and [`summarize()`](https://dplyr.tidyverse.org/reference/summarise.html).
Then, answer the question of if shorts are, on average, cheaper than other businesses for these data.

```{webr}
# insert code here
```

::: {.callout-tip collapse="true"}
## Solution

```{r}
#| echo: false
ecomm <- ecomm |>
  mutate(dummy = if_else(str_detect(title, "Short"), "short", "else"))
```

```{r}
ecomm |>
  group_by(dummy) |>
  summarize(mean_price = mean(price))
```

It appears that an eCommerce business that sells shorts was, on average, 8 dollars cheaper than other stores for these data.
:::

## Asking Permission

Just because you can use rvest tools to scrape data doesn't mean you should or have permission to do so.
We will use the `bow()` function from the polite package in R to introduce the client to the host and ask for permission to scrape.
Use the following code to ensure that we had permission to scrape Star Wars!
Note that the input for `bow()` is a character URL.

```{r}
#| warning: false
host <- "https://rvest.tidyverse.org/articles/starwars.html"

bow(host)
```

Indeed, we had permission to scrape that website!

Let's check out four more websites:

-   [espn.com](https://www.espn.com)
-   [x.com](https://x.com/)
-   [bettycrocker.com](https://www.bettycrocker.com/)
-   [zillow.com](https://www.zillow.com/)

```{r}
host1 <- "https://espn.com"
host2 <- "https://x.com/home"
host3 <- "https://www.bettycrocker.com"
host4 <- "https://www.zillow.com"

bow(host1)

bow(host2)

bow(host3)

bow(host4)
```

We do not have permission to scrape [espn.com](https://www.espn.com) or [x.com](https://x.com/).
We do have permission to scrape [bettycrocker.com](https://www.bettycrocker.com/) and [zillow.com](https://www.zillow.com/).

The takeaway is that, just because you can doesn't mean you should scrape the data!
