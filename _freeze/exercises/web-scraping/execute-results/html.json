{
  "hash": "d42344679d8fa9651a5ae2803f2b7367",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Scraping Star Wars movies and an eCommerce website\nformat: live-html\nengine: knitr\nwebr:\n  packages: \n    - tidyverse\n    - rvest\n    - polite\n  cell-options:\n    autorun: false\n  resources:\n    - https://rvest.tidyverse.org/articles/starwars.html\n    - https://raw.githubusercontent.com/data-science-with-r/data-tidying-import/main/exercises/data/items-80.csv\n---\n\n\n\n<!-- begin: webr fodder -->\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n#| echo: false\n#| output: false\noptions(\"readr.edition\" = 1)\n```\n:::\n\n\n\n<!-- end: webr fodder -->\n\nWe'll use the following packages for this programming exercise:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(polite)\n```\n:::\n\n\n\n## Star Wars\n\nFor this part of the programming exercise we'll scrape information on some Star Wars movies from <https://rvest.tidyverse.org/articles/starwars.html> and save it as a tidy data frame in R.\n\nFirst, though, we'll read the source code of the page and store it as an object we can access and parse in R.\n\n::: exercise\nRead the source code of the page and store it as `star_wars_html`.\n\n\n\n::: {.cell exercise='ex_1'}\n```{webr}\n#| exercise: ex_1\n___ <- ___(\"___\")\n```\n:::\n\n\n\n::: {.hint exercise=\"ex_1\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 1\n\nRead in the page source code with `read_html()` and pass the URL of the page, `https://rvest.tidyverse.org/articles/starwars.html` as the argument of this funtion.\n\n``` r\n___ <- read_html(\"https://rvest.tidyverse.org/articles/starwars.html\")\n```\n:::\n:::\n\n::: {.solution exercise=\"ex_1\"}\n::: {.callout-tip collapse=\"false\"}\n## Solution\n\nAssign the result of `read_html()` to `star_wars_html`.\n\n``` r\nstar_wars_html <- read_html(\"https://rvest.tidyverse.org/articles/starwars.html\")\n```\n:::\n:::\n:::\n\n### Titles\n\nBefore we get started, let's first take a note of the movie titles.\n\nScroll through the website and list the titles of the movies.\nThere are seven movie titles on this page.\nThey include:\n\n-   The Phantom Menace\n-   Attack of the Clones\n-   Revenge of the Sith\n-   A New Hope\n-   The Empire Strikes Back\n-   Return of the Jedi\n-   The Force Awakens\n\nThe next step is to identify the CSS selector that is associated with each title.\nWe'll use the [Selector Gadget](https://selectorgadget.com/) to help identify the CSS selectors associated with the items we're interested in scraping.\nSelector Gadget is an open source extension for the Chrome browser that helps users find elements of an webpage easier.\nGo to [this page](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en) to install the extension.\nWhen the Selector Gadget is installed, you should see its icon, ![](images/gadget.png), in your task bar.\n\nNext, we need to find the CSS selector that is associated with the titles of each Star Wars movie.\nClick the ![](images/gadget.png) icon and click on one of the titles.\nClick on another one.\n\n![](images/css-selector-1.png)\n\nNotice that the text you clicked on (that you wish to scrape) is highlighted in green.\nOther pieces of text with the same CSS selector are highlighted in yellow.\n\nWe can see that the text in green/yellow include the seven titles, and an additional element, the text that reads \"ON THIS PAGE\".\nThis last piece of text is not something we want, therefore we unselect it by clicking on it, which changes the highlight to red.\n\n![](images/css-selector-2.png)\n\nNow, we have our seven titles in green/yellow that we want to scrape.\nIn the bottom right corner, we can see the CSS selector associated with just these seven titles.\nWe will use this information to extract the titles from the page source code we saved earlier.\n\n::: exercise\nFirst, run the code below and inspect the result.\nThen, re-run with `html_text2()` instead of `html_text()` in the last step of the pipe.\nWhat is the difference between these functions?\nWhich is preferable, and why?\n\n\n\n::: {.cell setup='true' exercise='ex_2'}\n```{webr}\n#| setup: true\n#| exercise: ex_2\nstar_wars_html <- read_html(\"https://rvest.tidyverse.org/articles/starwars.html\")\n```\n:::\n\n::: {.cell exercise='ex_2'}\n```{webr}\n#| exercise: ex_2\ntitles <- star_wars_html |>\n  html_elements(\"#main h2\") |>\n  html_text()\n\ntitles\n```\n:::\n\n\n\n::: {.hint exercise=\"ex_2\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 1\n\nSwap `html_text()` with `html_text2()`.\n\n``` r\ntitles <- star_wars_html |>\n  html_elements(\"#main h2\") |>\n  html_text2()\n  \ntitles\n```\n:::\n:::\n\n::: {.solution exercise=\"ex_2\"}\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n`html_text()` returns the raw underlying text, including any white spaces, HTML code, etc. `html_text2()` tries to simulate how the text is displayed in the browser, i.e., cleans up the text.\nYou can find out more about these functions in the [function documentation](https://rvest.tidyverse.org/reference/html_text.html).\nIn this case, `html_text2()` is preferable.\n\n``` r\ntitles <- star_wars_html |>\n  html_elements(\"#main h2\") |>\n  html_text2()\n\ntitles\n```\n:::\n:::\n:::\n\n### Release dates\n\nNext, let's extract the release dates.\nWe can follow the same process as above.\n\nClick on a released date that you hope to scrape.\nClick on any yellow text you wish to exclude that has the same element tag.\nClick on a second released date.\nAgain, click on any yellow text you wish to exclude.\n\nOnce you completed this process, you should see an element tag for just the released dates of `section \\> p:nth-child(2)`.\n\n![](images/element-released.png)\n\n::: exercise\nUse this information to extract the seven released dates and save them as an object called `release_dates`.\n\n\n\n::: {.cell setup='true' exercise='ex_3'}\n```{webr}\n#| setup: true\n#| exercise: ex_3\nstar_wars_html <- read_html(\"https://rvest.tidyverse.org/articles/starwars.html\")\n```\n:::\n\n::: {.cell exercise='ex_3'}\n```{webr}\n#| exercise: ex_3\n___ <- star_wars_html |>\n  ___\n\nrelease_dates\n```\n:::\n\n\n\n::: {.hint exercise=\"ex_3\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 1\n\nExtract HTML elements with the CSS selector `\"section > p:nth-child(2)`\".\nWe use `html_elements()` and not `html_element()` since there are multiple release dates we want to extract.\n\n``` r\n___ <- star_wars_html |>\n  html_elements(\"section > p:nth-child(2)\") |>\n  ___\n\nrelease_dates\n```\n:::\n:::\n\n::: {.hint exercise=\"ex_3\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 2\n\nThen, clean up the text with `html_text2()`.\n\n``` r\n___ <- star_wars_html |>\n  html_elements(\"section > p:nth-child(2)\") |>\n  html_text2()\n\nrelease_dates\n```\n:::\n:::\n\n::: {.solution exercise=\"ex_3\"}\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nFinally, assign the resulting vector to `release_dates`, and view the contents of `release_dates`.\n\n``` r\nrelease_dates <- star_wars_html |>\n  html_elements(\"section > p:nth-child(2)\") |>\n  html_text2()\n\nrelease_dates\n```\n:::\n:::\n:::\n\n### Data frame of titles and release dates\n\nNow we're ready to put everything we scraped together in a data frame, thus converting unstructured data from the web into a tidy data frame in R.\n\n::: exercise\nCreate a tibble called `star_wars` with two columns: `title` and `release_date`.\nThis tibble should have seven rows, one for each movie scraped, and contain information from the `titles` and `release_dates` vectors you previously created.\nConfirm that you have correctly created the tibble by printing it out.\n\n\n\n::: {.cell setup='true' exercise='ex_4'}\n```{webr}\n#| setup: true\n#| exercise: ex_4\nstar_wars_html <- read_html(\"https://rvest.tidyverse.org/articles/starwars.html\")\ntitles <- star_wars_html |>\n  html_elements(\"#main h2\") |>\n  html_text2()\nrelease_dates <- star_wars_html |>\n  html_elements(\"section > p:nth-child(2)\") |>\n  html_text2()\n```\n:::\n\n::: {.cell exercise='ex_4'}\n```{webr}\n#| exercise: ex_4\n___\n```\n:::\n\n\n\n::: {.hint exercise=\"ex_4\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 1\n\nCreate a tibble with `tibble()` and call it `star_wars`.\nThis tibble will have two columns (variables), we define these separated by a comma in the `tibble()` call.\n\n``` r\nstar_wars <- tibble(\n  ___ = ___,\n  ___ = ___\n)\n```\n:::\n:::\n\n::: {.hint exercise=\"ex_4\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 2\n\nThe variables will be called `tibble` and `release_date`.\n\n``` r\nstar_wars <- tibble(\n  title = ___,\n  release_date = ___\n)\n```\n:::\n:::\n\n::: {.solution exercise=\"ex_4\"}\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nWe assign the vectors `titles` and `release_dates` that we had previous extracted and stored to these variables.\nAnd we finally view the contents of the new data frame we created, `star_wars`.\n\n``` r\nstar_wars <- tibble(\n  title = titles,\n  release_date = release_dates\n)\n\nstar_wars\n```\n:::\n:::\n:::\n\n## Working with scraped eCommerce pages\n\nIn the last Code Along activity, you wrote a function to scrape eCommerce data from multiple pages.\nIn this part of the programming exercise, we are going to practice working with numerical and text data that you scraped from this website.\n\nRun the following code to load these data from a CSV file called `items-80.csv`, and save it as `ecomm`.\n\n\n\n::: {.cell}\n```{webr}\necomm <- read_csv(\"items-80.csv\")\n```\n:::\n\n\n\n::: exercise\nTake a glimpse at the `ecomm` data frame.\nHow many observations does the data frame have?\nWhat does each observation represent?\nHow many variables does the data frame have?\nWhat are they?\n\n\n\n::: {.cell setup='true' exercise='ex_5'}\n```{webr}\n#| setup: true\n#| exercise: ex_5\necomm <- read_csv(\"items-80.csv\")\n```\n:::\n\n::: {.cell exercise='ex_5'}\n```{webr}\n#| exercise: ex_5\n___\n```\n:::\n\n\n\n::: {.solution exercise=\"ex_5\"}\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n`ecomm` has 80 observations, each observation is an item being sold on the eCommerce website.\nAnd `ecomm` has 3 variables, these are `title`, `url` and `price`.\n\n``` r\nglimpse(ecomm)\n```\n:::\n:::\n:::\n\n::: exercise\nStart by making a histogram of price using tools from the **ggplot2** package in the **tidyverse**.\n\n\n\n::: {.cell setup='true' exercise='ex_6'}\n```{webr}\n#| setup: true\n#| exercise: ex_6\necomm <- read_csv(\"items-80.csv\")\n```\n:::\n\n::: {.cell exercise='ex_6'}\n```{webr}\n#| exercise: ex_6\n___\n```\n:::\n\n\n\n::: {.hint exercise=\"ex_6\"}\n::: {.callout-note collapse=\"false\"}\n## Hint\n\nUse the functions `ggplot()` and `geom_histogram()` to make your plot.\nPlay around with the binwidth, and add appropriate labels.\n:::\n:::\n\n::: {.hint exercise=\"ex_6\"}\n::: {.callout-note collapse=\"false\"}\n## Hint\n\nBelow is the general structure you can use to create this plot.\n\n``` r\nggplot(___, aes(x = ___)) +\n  geom_histogram(binwidth = ___) + \n  labs(\n    title = \"___\",\n    x = \"___\",\n    y = \"___\"\n  )\n```\n:::\n:::\n\n::: {.solution exercise=\"ex_6\"}\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nA binwidth of 5 seems reasonable.\nYou can try other values as well.\n\n``` r\nggplot(ecomm, aes(x = price)) +\n  geom_histogram(binwidth = 5) + \n  labs(\n    title = \"Price of eCommerce data\",\n    x = \"Price\",\n    y = \"Count\"\n  )\n```\n:::\n:::\n:::\n\nNext, you'll work with text data to discover more patterns.\nSuppose you're interested in prices of shorts on this eCommerce site, but the titles of items that would be consideted \"shorts\" can be all over te place.\n\n::: exercise\nCreate a new variable, `shorts`, that takes the value `TRUE` if the `title` of an item contains the word `short` or `Short`, and `FALSE` otherwise.\nSave the data frame with this new variable.\n\n\n\n::: {.cell setup='true' exercise='ex_7'}\n```{webr}\n#| setup: true\n#| exercise: ex_7\necomm <- read_csv(\"items-80.csv\")\n```\n:::\n\n::: {.cell exercise='ex_7'}\n```{webr}\n#| exercise: ex_7\necomm <- ecomm |>\n  ___(___ = ___)\n```\n:::\n\n\n\n::: {.hint exercise=\"ex_7\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 1\n\nUse the `mutate()` function to create the new variable.\n\n``` r\necomm <- ecomm |>\n  mutate(___ = ___)\n```\n:::\n:::\n\n::: {.hint exercise=\"ex_7\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 2\n\nUse the `if_else()` function to check that the `title` contains the specified text strings, and set the value of the new variable `shorts` to `TRUE` if yes, and `FALSE` if no.\n\n``` r\necomm <- ecomm |>\n  mutate(shorts = if_else(___, TRUE, FALSE))\n```\n:::\n:::\n\n::: {.hint exercise=\"ex_7\"}\n::: {.callout-note collapse=\"false\"}\n## Hint 3\n\nUse the `str_detect()` function to determine whether `title` contains the text string `short` or `Short`, where \"or\" is denoted with `|`.\n\n``` r\nstr_detect(title, \"short|Short\")\n```\n:::\n:::\n\n::: {.solution exercise=\"ex_7\"}\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nPutting it altogether...\n\n``` r\necomm <- ecomm |>\n  mutate(shorts = if_else(str_detect(title, \"short|Short\"), TRUE, FALSE))\n```\n:::\n:::\n:::\n\nYou just created an indicator variable!\n\n::: exercise\nUsing this variable, evaluate how the average price of shorts compare to the average price of other items.\nHow do the standard deviations of the prices of these two groups of items compare?\n\n\n\n::: {.cell setup='true' exercise='ex_8'}\n```{webr}\n#| setup: true\n#| exercise: ex_8\necomm <- read_csv(\"items-80.csv\")\necomm <- ecomm |>\n  mutate(shorts = if_else(str_detect(title, \"short|Short\"), TRUE, FALSE))\n```\n:::\n\n::: {.cell exercise='ex_8'}\n```{webr}\n#| exercise: ex_8\n___\n```\n:::\n\n\n\n::: {.hint exercise=\"ex_8\"}\n::: {.callout-note collapse=\"false\"}\n## Hint\n\nUse `group_by()` and `summarize()` for this task.\n\n``` r\necomm |>\n  group_by(___) |>\n  summarize(___)\n```\n:::\n:::\n\n::: {.solution exercise=\"ex_8\"}\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nShorts, on average, sell for \\$8 lower than other items.\nThe prices of shorts are less variable (lower standard deviation) than other items.\n\n``` r\necomm |>\n  group_by(shorts) |>\n  summarize(\n    mean = mean(price),\n    sd = sd(price)\n  )\n```\n:::\n:::\n:::\n\n## Asking for permission\n\nJust because you have the tools to scrape data doesn't mean you should or have permission to do so.\nWe will use the `bow()` function from the polite package in R to introduce the client to the host and ask for permission to scrape.\nUse the following code to ensure that we had permission to scrape Star Wars!\nNote that the input for `bow()` is a URL, provided as a character string.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbow(\"https://rvest.tidyverse.org/articles/starwars.html\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://rvest.tidyverse.org/articles/starwars.html\n    User-agent: polite R package\n    robots.txt: 1 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n:::\n\n\n\nIndeed, we had permission to scrape that website!\n\n::: exercise\nDetermine whether you can scrape data from the following websites based on the output shown below.\n\n-   [espn.com](https://www.espn.com)\n-   [x.com](https://x.com/)\n-   [bettycrocker.com](https://www.bettycrocker.com/)\n-   [zillow.com](https://www.zillow.com/)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbow(\"https://espn.com\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://espn.com\n    User-agent: polite R package\n    robots.txt: 114 rules are defined for 11 bots\n   Crawl delay: 5 sec\n  The path is not scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(\"https://x.com/home\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://x.com/home\n    User-agent: polite R package\n    robots.txt: 24 rules are defined for 7 bots\n   Crawl delay: 5 sec\n  The path is not scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(\"https://www.bettycrocker.com\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://www.bettycrocker.com\n    User-agent: polite R package\n    robots.txt: 15 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(\"https://www.zillow.com\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://www.zillow.com\n    User-agent: polite R package\n    robots.txt: 244 rules are defined for 8 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nWe do not have permission to scrape [espn.com](https://www.espn.com) or [x.com](https://x.com/).\nWe do have permission to scrape [bettycrocker.com](https://www.bettycrocker.com/) and [zillow.com](https://www.zillow.com/).\n:::\n:::\n\n## Takeaway\n\nThe takeaway is that -- just because you can doesn't mean you should scrape the data!\n\n\n## Reflection questions\n\n-   Why do you believe we are studying the content above?\n\n\n```{=html}\n<textarea rows=\"8\" cols=\"80\">\n</textarea>\n```\n\n-   List three topics that you feel from the content above.\n\n\n```{=html}\n<textarea rows=\"8\" cols=\"80\">\n</textarea>\n```\n\n-   List three topics that you feel could use more practice (or you are more curious about) from the content above.\n\n\n```{=html}\n<textarea rows=\"8\" cols=\"80\">\n</textarea>\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}