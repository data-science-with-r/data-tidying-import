{
  "hash": "86c231ed211550ea9e03f7ea80d49b5e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scraping Star Wars movies and an E-Commerce website\"\nformat: \n  live-html:\n    toc: true\nengine: knitr\nwebr:\n  packages: \n    - tidyverse\n    - rvest\n    - polite\n  cell-options:\n    autorun: false\n  resources:\n    - https://rvest.tidyverse.org/articles/starwars.html\n    - https://raw.githubusercontent.com/data-science-with-r/data-tidying-import/main/exercises/data/items-80.csv\n---\n\n\n<!-- begin: webr fodder -->\n\n::: {.cell}\n\n:::\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n#| echo: false\n#| output: false\noptions(\"readr.edition\" = 1)\n```\n:::\n\n\n<!-- end: webr fodder -->\n\n\n\n\n\nWe'll use the following packages for this programming exercise:\n\n\n::: {.cell}\n```{webr}\n#| message: false\n#| warning: false\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(polite)\n```\n:::\n\n\n## Star Wars\n\nScraping data is defined as extracting data from one source to another.\nWe are going to use the following [website](https://rvest.tidyverse.org/articles/starwars.html) to practice scraping data; taking data on the website and transporting it into a more workable format with R code.\n\n### Scraping titles\n\nPlease visit the [website](https://rvest.tidyverse.org/articles/starwars.html) and identify the movie titles.\nWhat are they?\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nThere are seven movie titles on this page.\nThey include:\n\n-- The Phantom Menace\n\n-- Attack of the Clones\n\n-- Revenge of the Sith\n\n-- A New Hope\n\n-- The Empire Strikes Back\n\n-- Return of the Jedi\n\n-- The Force Awakens\n:::\n\nThe next step is to identify the html element that is associated with each title.\nTo help us, we can use [Selector Gadget](https://selectorgadget.com/).\nSelector Gadget is an open source Chrome Extension that helps users find elements of an html page easier.\nThis demonstration will use Selector Gadget.\nTo install Selector Gadget, please go to the following [page](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en&pli=1).\nWhen the Selector Gadget is installed, you should see ![](images/gadget.png) in your task bar.\n\n#### Start by reading a HTML page with read_html()\n\nFirst, we need to reference the website.\nWe can do this by using `read_html()` from the rvest package.\nRun the following code below to do so.\n\n\n::: {.cell}\n```{webr}\nstarwars <- read_html(\"https://rvest.tidyverse.org/articles/starwars.html\")\n```\n:::\n\n\n#### Find the element\n\nNext, we need to find the element that is associated with the titles of each Star Wars movie.\nClick the ![](images/gadget.png) icon and click on one of the titles. Click on another one.\n\n![](images/element1.png)\n\nNotice that the next you click on (that you wish to scrape) is highlighted in green. Text in yellow show what else has the same element tag as the text you are clicking on.\n\nWe can see that the text in green/yellow include the seven titles, and the \"ON THIS PAGE\" text. If we don't want to scrape certain text (e.g., the menu title), we can click it, changing the yellow box to red.\n\n![](images/element2.png)\n\nNow, we have our seven titles in green/yellow that we want to scrape. In the bottom right corner, we can see the element associated with just these seven titles. We will use this element information to scrape the titles below.\n\n\n::: {.cell}\n```{webr}\nstarwars |>\n  html_elements(\"#main h2\") |>\n  html_text2()\n```\n:::\n\n\nChange the above code from `html_text2()` to `html_text()` to best understand the difference between these two functions. \n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nSee the help file for each function [here](https://rvest.tidyverse.org/reference/html_text.html).\n\nhtml_text() returns raw underlying text, including white pace and other symbols. html_text2() tries to simulate how the text is displayed.\n:::\n\n### Scraping release dates\n\nSuppose now we want to scrape release dates instead. We can follow the same process as above. \n\nClick on a released date that you hope to scrape. Click on any yellow text you wish to exclude that has the same element tag. Click on a second released date. Again, click on any yellow text you wish to exclude.\n\nOnce you completed this process, you should see an element tag for just the released dates of *section > p:nth-child(2)*. Use this information to scrape the seven released dates below.\n\n![](images/element-released.png)\n\n\n::: {.cell}\n```{webr}\n#insert code here\n```\n:::\n\n\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nstarwars |>\n  html_elements(\"section > p:nth-child(2)\") |>\n  html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Released: 1999-05-19\" \"Released: 2002-05-16\" \"Released: 2005-05-19\"\n[4] \"Released: 1977-05-25\" \"Released: 1980-05-17\" \"Released: 1983-05-25\"\n[7] \"Released: 2015-12-11\"\n```\n\n\n:::\n:::\n\n:::\n\n## Working with scraped eCommerce pages\n\n\n::: {.cell}\n```{webr}\necomm <- read_csv(\"items-80.csv\")\n```\n:::\n\n\nIn the last code along activity, you wrote a function to scrape eCommerce data from multiple pages.\nHere, we are going to practice working with numerical and text data.\nThe data set name is called `ecomm`.\n\nTake a glimpse at the `ecomm` data set below to familiarize yourself with the variables.\nWhat are they?\n\n\n::: {.cell}\n```{webr}\n# insert code here\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(ecomm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 80\nColumns: 3\n$ title <chr> \"Abominable Hoodie\", \"Adrienne Trek Jacket\", \"Aeon Capri\", \"Aero…\n$ url   <chr> \"https://www.scrapingcourse.com/ecommerce/product/abominable-hoo…\n$ price <dbl> 69.0, 57.0, 48.0, 24.0, 74.0, 7.0, 45.0, 69.0, 40.0, 42.0, 34.0,…\n```\n\n\n:::\n:::\n\n\nThere is a `title`, `url` and `price` variable.\n:::\n\nNow, let's start by making a histogram of price using tools from the `ggplot` package in the `tidyverse`.\nHint: Use the functions [ggplot()](https://ggplot2.tidyverse.org/reference/ggplot.html) and [geom_histogram()](https://ggplot2.tidyverse.org/reference/geom_histogram.html) to make your visualization below.\nNote: Play around with the binwidth, and add appropriate labels if able!\n\n\n::: {.cell}\n```{webr}\n# insert code here\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ecomm, aes(x = price)) +\n  geom_histogram(binwidth = 5) + \n  labs(\n    title = \"Price of eCommerse scraped data\",\n    x = \"Price\"\n  )\n```\n\n::: {.cell-output-display}\n![](web_scraping_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n:::\n\nNow, let's work with some of the text data to discover more patterns.\nSuppose you wonder if an eCommerce business that sells shorts is cheaper than others.\nFirst, we need to find how many listings contain the word \"short\".\nWe can do this using [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) from the `stringR` package.\nRun the following code.\nWhat is it doing?\n\n\n::: {.cell}\n```{webr}\necomm <- ecomm |>\n  mutate(dummy = ifelse(str_detect(title, \"Short\"), \"short\", \"else\"))\n\necomm\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nThis code is using `mutate()` to create a new dummy variable called `dummy`.\nIt is using a combination of `ifelse()` and `str_detect()` to look into the title column, and see if the string \"Short\" was detected or not.\nIf yes, the new variable `dummy` will contain \"short\" for that row.\nIf no, the new variable will contain \"else\".\n:::\n\nWe just made a dummy (or indicator) variable!\nThis is going to help us see if eCommerce businesses that sell shorts are, on average, more or less expensive than others.\nUse this variable, along with the `dplyr` functions of [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) and [`summarize()`](https://dplyr.tidyverse.org/reference/summarise.html).\nThen, answer the question of if shorts are, on average, cheaper than other businesses for these data.\n\n\n::: {.cell}\n```{webr}\n# insert code here\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\necomm |>\n  group_by(dummy) |>\n  summarize(mean_price = mean(price))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  dummy mean_price\n  <chr>      <dbl>\n1 else        43.7\n2 short       35.7\n```\n\n\n:::\n:::\n\n\nIt appears that an eCommerce business that sells shorts was, on average, 8 dollars cheaper than other stores for these data.\n:::\n\n## Asking Permission\n\nJust because you can use rvest tools to scrape data doesn't mean you should or have permission to do so.\nWe will use the `bow()` function from the polite package in R to introduce the client to the host and ask for permission to scrape.\nUse the following code to ensure that we had permission to scrape Star Wars!\nNote that the input for `bow()` is a character URL.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhost <- \"https://rvest.tidyverse.org/articles/starwars.html\"\n\nbow(host)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://rvest.tidyverse.org/articles/starwars.html\n    User-agent: polite R package\n    robots.txt: 1 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n:::\n\n\nIndeed, we had permission to scrape that website!\n\nLet's check out four more websites:\n\n-   [espn.com](https://www.espn.com)\n-   [x.com](https://x.com/)\n-   [bettycrocker.com](https://www.bettycrocker.com/)\n-   [zillow.com](https://www.zillow.com/)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhost1 <- \"https://espn.com\"\nhost2 <- \"https://x.com/home\"\nhost3 <- \"https://www.bettycrocker.com\"\nhost4 <- \"https://www.zillow.com\"\n\nbow(host1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://espn.com\n    User-agent: polite R package\n    robots.txt: 114 rules are defined for 11 bots\n   Crawl delay: 5 sec\n  The path is not scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(host2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://x.com/home\n    User-agent: polite R package\n    robots.txt: 24 rules are defined for 7 bots\n   Crawl delay: 5 sec\n  The path is not scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(host3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://www.bettycrocker.com\n    User-agent: polite R package\n    robots.txt: 15 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(host4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://www.zillow.com\n    User-agent: polite R package\n    robots.txt: 244 rules are defined for 8 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n:::\n\n\nWe do not have permission to scrape [espn.com](https://www.espn.com) or [x.com](https://x.com/).\nWe do have permission to scrape [bettycrocker.com](https://www.bettycrocker.com/) and [zillow.com](https://www.zillow.com/).\n\nThe takeaway is that, just because you can doesn't mean you should scrape the data!\n",
    "supporting": [
      "web_scraping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}