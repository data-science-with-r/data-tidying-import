{
  "hash": "1f47707d3ffd3321b518660a378f1c63",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scraping Star Wars\"\ntoc: true\nwebr:\n  packages: ['tidyverse', 'scales', 'rvest', 'polite', 'curl', 'stringr']\nfilters:\n  - webr\n---\n\n\n## Star Wars\n\n\n::: {.cell}\n\n:::\n\n\n\n```{webr-r}\n#| echo: false\necomm <- read_csv(\"data/items-80.csv\")\n```\n\n\n::: {.cell}\n\n:::\n\n\nScraping data is defined as extracting data from one source to another. We are going to use the following [website](https://rvest.tidyverse.org/articles/starwars.html) to practice scraping data; taking data on the website and transporting it into a more workable format with R code.\n\n### Scraping titles\n\nPlease visit the [website](https://rvest.tidyverse.org/articles/starwars.html) and identify the movie titles. What are they?\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nThere are seven movie titles on this page. They include: \n\n-- The Phantom Menace\n\n-- Attack of the Clones\n\n-- Revenge of the Sith\n\n-- A New Hope\n\n-- The Empire Strikes Back\n\n-- Return of the Jedi\n\n-- The Force Awakens\n:::\n\nThe next step is to identify the html element that is associated with each title. To help us, we can use [Selector Gadget](https://selectorgadget.com/). Selector Gadget is an open source Chrome Extension that helps users find elements of an html page easier. This demonstration will use Selector Gadget. To install Selector Gadget, please go to the following [page](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en&pli=1). When the Selector Gadget is installed, you should see ![](images/gadget.png) in your task bar.\n\n#### Start by reading a HTML page with read_html()\n\nFirst, we need to reference the website. We can do this by using `read_html()` from the rvest package. Run the following code below to do so.\n\n```{webr-r}\nstarwars <- read_html(\"https://rvest.tidyverse.org/articles/starwars.html\")\n```\n\n#### Find the element\n\nNext, we need to find the element that is associated with the titles of each Star Wars movie. Click the ![](images/gadget.png) icon and click on one of the titles.\n\n![](images/h2.png)\n\nIn the red outline, you will see the element `section`. Click on a different title. You will again see the element `section`. We can use this information to pull the titles from the website using the following code:\n\n```{webr-r}\nfilms <- starwars |>\n  html_elements(\"section\")\n\nfilms\n```\n\nNotice how seven nodes (titles) had the same element `section`. You can see the title of the movie, along with other data, in each row. Among this output, you can see that each data-id has an `h2` element. We also saw h2 when using the selector gadget on the title. Now, let's pull just the information associated with the `h2` element.\n```{webr-r}\n\ntitle <- films |> \n  html_element(\"h2\") |> \n  html_text2()\n\ntitle\n```\n\nAnother way to get title information is to take them directly from the `starwars` html using `html_elements(\"h2\")`. See the following code below.\n\n```{webr-r}\nstarwars |>\n  html_elements(\"h2\")\n```\n\nNotice that this has 8 nodes, instead of 7, as the data \"On this page\" does not have the element `section` tag. \n\n## Working with scraped eCommerce pages\n\nIn the last code along activity, you wrote a function to scrape eCommerce data from multiple pages. Here, we are going to practice working with numerical and text data. The data set name is called `ecomm`. \n\n```{webr-r}\n#| context: setup\ndownload.file(\n  \"https://raw.githubusercontent.com/ElijahMeyer3/Coursera/main/data/items-80.csv\",\n  \"ecomm.csv\"\n)\n\noptions(readr.show_progress = FALSE)\n```\n\nTake a glimpse of the `ecomm` data set below to familiarize yourself with the variables. What are they?\n\n```{webr-r}\n\n```\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(ecomm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 80\nColumns: 3\n$ title <chr> \"Abominable Hoodie\", \"Adrienne Trek Jacket\", \"Aeon Capri\", \"Aero…\n$ url   <chr> \"https://www.scrapingcourse.com/ecommerce/product/abominable-hoo…\n$ price <dbl> 69.0, 57.0, 48.0, 24.0, 74.0, 7.0, 45.0, 69.0, 40.0, 42.0, 34.0,…\n```\n\n\n:::\n:::\n\n\nThere is a `title`, `url` and `price` variable.\n:::\n\nNow, let's start by making a histogram of price using tools from the `ggplot` package in the `tidyverse`. Hint: Use the functions [ggplot()](https://ggplot2.tidyverse.org/reference/ggplot.html) and [geom_histogram()](https://ggplot2.tidyverse.org/reference/geom_histogram.html) to make your visualization below. Note: Play around with the binwidth, and add appropriate labels if able!\n\n```{webr-r}\n\n```\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\necomm |>\n ggplot(\n   aes(x = price)\n ) +\n  geom_histogram(binwidth = 5) + \n  labs(\n    title = \"Price of eCommerse scraped data\",\n    x = \"Price\"\n  )\n```\n\n::: {.cell-output-display}\n![](web_scraping_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n:::\n\nNow, let's work with some of the text data to discover more patterns. Suppose you wonder if an eCommerce business that sells shorts are cheaper than others. First, we need to find how many listings contain the word \"short\". We can do this using [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) for the `stringR` package. Run the following code. What is it doing? \n\n```{webr-r}\necomm <- ecomm |>\n  mutate(dummy = ifelse(\n    str_detect(\n      title, \"Short\"), \"short\", \"else\"))\n\necomm\n```\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\nThis code is using `mutate()` to create a new dummy variable called `dummy`. It is using a combination of `ifelse()` and `str_detect()` to look into the title column, and see if the string \"Short\" was detected or not. If yes, the new variable `dummy` will contain \"short\" for that row. If no, the new variable will contain \"else\".\n:::\n\nWe just made a dummy (or indicator) variable! This is going to help us see if eCommerce businesses that sell shorts are, on average, more or less expensive than others. Use this variable, along with the `dplyr` functions of [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) and [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html). Then, answer the question of if shorts are, on average, cheaper than other businesses for these data.\n\n```{webr-r}\n#insert code here\n```\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\necomm |>\n  group_by(dummy) |>\n  summarise(mean_price = mean(price))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  dummy mean_price\n  <chr>      <dbl>\n1 else        43.7\n2 short       35.7\n```\n\n\n:::\n:::\n\nIt appears that an eCommerce business that sells shorts was, on average, 8 dollars cheaper than other stores for these data.\n\n:::\n## Asking Permission \n\nJust because you can use rvest tools to scrape data doesn't mean you should or have permission to do so. We will use the `bow()` function from the polite package in R to introduce the client to the host and ask for permission to scrape. Use the following code to ensure that we had permission to scrape Star Wars! Note that the input for `bow()` is a character URL. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhost <- \"https://rvest.tidyverse.org/articles/starwars.html\"\n\nbow(host)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://rvest.tidyverse.org/articles/starwars.html\n    User-agent: polite R package\n    robots.txt: 1 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n:::\n\n\n\nIndeed, we had permission to scrape that website! \n\nLet's check out four more websites: \n\n-   https://espn.com \n-   https://x.com/home\n-   https://www.bettycrocker.com\n-   https://www.zillow.com\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhost1 <- \"https://espn.com\"\nhost2 <- \"https://x.com/home\"\nhost3 <- \"https://www.bettycrocker.com\"\nhost4 <- \"https://www.zillow.com\"\n\nbow(host1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://espn.com\n    User-agent: polite R package\n    robots.txt: 114 rules are defined for 11 bots\n   Crawl delay: 5 sec\n  The path is not scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(host2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://x.com/home\n    User-agent: polite R package\n    robots.txt: 24 rules are defined for 7 bots\n   Crawl delay: 5 sec\n  The path is not scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(host3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://www.bettycrocker.com\n    User-agent: polite R package\n    robots.txt: 15 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\nbow(host4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://www.zillow.com\n    User-agent: polite R package\n    robots.txt: 244 rules are defined for 8 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n:::\n\n\nWe do not have permission to scrape espn.com or x.com. We do have permission to scrape bettycrocker.com and zillow.com.\n\nThe takeaway is that, just because you can doesn't mean you are allowed to scrape data!\n",
    "supporting": [
      "web_scraping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}